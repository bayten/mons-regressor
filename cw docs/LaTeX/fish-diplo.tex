\documentclass[12pt,fleqn]{article}
\usepackage[cp1251]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage{amsfonts,amssymb,amsmath,mathrsfs,amsthm, array}
\usepackage[russian]{babel}
\usepackage{graphicx}
\usepackage[footnotesize]{caption2}
\usepackage{indentfirst}
%\usepackage[ruled,section]{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{algorithm}
%\usepackage[all]{xy}

% Параметры страницы
\textheight=24cm
\textwidth=16cm
\oddsidemargin=5mm
\evensidemargin=-5mm
\marginparwidth=36pt
\topmargin=-1cm
\footnotesep=3ex
%\flushbottom
\raggedbottom
\tolerance 3000
% подавить эффект "висячих стpок"
\clubpenalty=10000
\widowpenalty=10000
\renewcommand{\baselinestretch}{1.1}
\renewcommand{\baselinestretch}{1.5} %для печати с большим интервалом


\def\algorithmicrequire{\textbf{Вход:}}
\def\algorithmicensure{\textbf{Выход:}}
\def\algorithmicif{\textbf{если}}
\def\algorithmicthen{\textbf{то}}
\def\algorithmicelse{\textbf{иначе}}
\def\algorithmicelsif{\textbf{иначе если}}
\def\algorithmicfor{\textbf{для}}
\def\algorithmicforall{\textbf{для всех}}
\def\algorithmicdo{}
\def\algorithmicwhile{\textbf{пока}}
\def\algorithmicrepeat{\textbf{повторять}}
\def\algorithmicuntil{\textbf{пока}}
\def\algorithmicloop{\textbf{цикл}}
% переопределение стиля комментариев
\def\algorithmiccomment#1{\quad// {\sl #1}}


\begin{document}

\begin{titlepage}
\begin{center}
    Московский государственный университет имени М. В. Ломоносова

    \bigskip
    \includegraphics[width=50mm]{msu.eps}

    \bigskip
    Факультет Вычислительной Математики и Кибернетики\\
    Кафедра Математических Методов Прогнозирования\\[10mm]

    \textsf{\large\bfseries
        КУРСОВАЯ РАБОТА СТУДЕНТА 317 ГРУППЫ\\[7mm]
        \normalsize << Логические корректоры в задаче восстановления регрессии >> 
    }\\[13mm]

    \begin{flushright}
        \parbox{0.5\textwidth}{
            Выполнил:\\
            студент 3 курса 317 группы\\
            \emph{Байтеков Никита Вячеславович}\\[5mm]
            Научный руководитель:\\
            д.ф-м.н., доцент\\
            \emph{Дюкова Елена Всеволодовна}
        }
    \end{flushright}

    \vspace{\fill}
    Москва, 2017
\end{center}
\end{titlepage}

\newpage
\renewcommand{\contentsname}{Содержание}
\tableofcontents

\newpage

\section{Введение}
Исследуется множество объектов $M$, которые описываются набором целочисленных признаков $\lbrace x_1, ..., x_n \rbrace$. Дано обучающее множество объектов $ T = \lbrace S_1, ..., S_m \rbrace $ из $M$, такое, что для каждого обучающего объекта(прецедента) известно значение <<ответа>> из числового множества ответов $Y$. Требуется построить алгоритм распознавания $A_T : M \to Y$, ставящий в соответствие произвольному объекту из $M$ ответ из $Y$. В случае, когда $Y = \mathbb{R}$, решается задача восстановления регрессии, а при $Y = \lbrace 1, \dots , l \rbrace, l \geq 2$ --- задача классификации.

В работе рассматривается задача восстановления регрессии, для решения которой существует много методов. Чаще всего используется многомерная линейная регрессия, когда зависимость ответа от признаков предполагается линейной. Для решения задачи также используются адаптации методов классификации. Хорошо показывает себя классификация по методу ближайших соседей, который исходит из предположения, что у похожих объектов будут похожие ответы. Также рассматриваются вариации решающих деревьев, которые стабильно показывают высокое качество полученных решений.

Одной из современных моделей классификации является логический корректор \cite{new_pprok} \cite{sotnezov11}, в основе которого лежат идеи как логического, так и алгебраического подходов\cite{alg_meth}. Основа классического логического подхода -- использование корректных алгоритмов, базирующихся на построении корректных элементарных классификаторов. В алгебраическом подходе несколько распознающих алгоритмов объединяются в один для взаимной коррекции ошибок. Логические корректоры, хорошо зарекомендовавшие себя при решении прикладных задач, базируются на построении корректных наборов из произвольных элементарных классификаторов. В \cite{lyubimtseva14} разработан стохастический вариант логического корректора MONS. Целью данной работы является адаптация этого алгоритма к задаче восстановления регрессии.

Рассмотрим задачу классификации и введём основные определения. Пусть обучающее множество $T$ разбито на $l$ непересекающихся множеств $\lbrace K_1, ..., K_l \rbrace$, называемых {\it классами}, и для каждого прецедента известно, к какому из классов он принадлежит. Обозначим множество прецедентов из $T$, принадлежащих некоторому классу $K, K \in \lbrace K_1, ..., K_l \rbrace$, как $T \cap K$, а $T \cap \overline{K}$ --- как множество прецедентов из $T$, не принадлежащих классу $K$. Пусть $H = \lbrace x_{j_1}, ..., x_{j_r} \rbrace$ --- набор различных признаков, $\sigma = \lbrace \sigma_1, ..., \sigma_r \rbrace$ --- набор допустимых значений этих признаков, где $\sigma_q$ - допустимое значение признака $x_{j_q}, q=1,\dots, r$. 

Пара $(H,\sigma)$ называется {\it элементарным классификатором(эл.кл.)} Число $r$ называется {\it рангом} эл.кл.$(H,\sigma)$. Обозначим через $x_j(S)$ значение признака $x_j$ на объекте $S$ и через $H(S)$ вектор
значений признаков $(x_{j_1}(S), \dots , x_{j_r}(S))$. Говорят, что эл.кл. $(H, \sigma)$ {\it выделяет} объект $S$, если $H(S) = \sigma$. Эл.кл. $(H, \sigma)$ считается {\it корректным для класса} $K$, если не существует двух выделяемых эл.кл. $(H, \sigma)$, таких, что $S' \in K, S'' \notin K$.

Пусть имеется упорядоченный {\it набор эл.кл.} $U = ((H_1 , \sigma_1 ), \dots, (H_d ,\sigma_d))$. Набор $U$ ставит в соответствие объекту $S$ из $M$ бинарный вектор $U(S) = ([H_1(S) = \sigma_1 ], \dots , [H_d(S) = \sigma_d])$, который называется {\it откликом} набора эл.кл. $U$ на объекте $S$ (здесь через $[p]$ обозначается предикат, принимающий 1, если выражение $p$ истинно, и 0 -- в противном случае) Набор эл.кл. $U$ называется {\it монотонно корректным для класса} $K$, где $K \in \lbrace K_1, ..., K_l \rbrace$, если $\forall (S', S''), S' \in K, S'' \notin K$ существует монотонная функция алгебры логики $F_{U,K}$: $F_{U,K}(U(S')) \neq F_{U,K} (U(S''))$. В таком случае, функция $F_{U,K}$ называется {\it монотонно корректирующей} для $U$. Монотонно корректный набор $U$ для класса $K$ является {\it неприводимым}, если любой набор эл.кл., являющийся подмножеством $U$, не является монотонно корректным. Неприводимый набор для класса $K$ является {\it минимальным}, если не существует монотонно корректных наборов для класса $K$ меньшей мощности.

В настоящей работе разработан алгоритм MONS-Reg, решающий задачу восстановления регрессии на основе модификации стохастического логического корректора MONS. Проведено тестирование алгоритма MONS-Reg на реальных задачах.

\newpage
\section{Логические корректоры в задаче классификации}
\subsection{Алгоритм MON}
Одним из первых логических корректоров, показывающих хорошие результаты, является алгоритм MON \cite{sotnezov09}, который использовал монотонно корректные наборы, состоящие из эл.кл. ранга 1.

Процесс работы корректора MON разбивается на два этапа: обучение модели и распознавание. На этапе обучения для каждого класса $K$, $K \in \lbrace K_1, ..., K_l \rbrace$, распознающий алгоритм $A$ строит семейство монотонно корректных наборов эл.кл. $W_A(K)$. При распознавании для каждого объекта $S$ вычисляется оценка принадлежности этого объекта к каждому из классов $K$ по формуле:
\[
\Gamma_K (S) = \frac{1}{|W_A(K)|} \sum_{U \in W_A(K)} \frac{1}{|T \cap K|} \sum_{S' \in T \cap K} \delta_U(S, S').
\]
где $\delta_U(S, S')$ называется {\it функцией голосования} и определяется как:
\[
\delta_U(S, S') = \begin{cases}
1, & \text{если $U(S) \succeq U(S')$;} \\
0, & \text{иначе.}
\end{cases}
\]
Алгоритм относит неизвестный объект S к тому классу, для которого оценка принадлежности будет максимальной.

\subsection{Алгоритм MONS}
Алгоритм MONS является улучшенной моделью алгоритма MON, который использует т.н. {\it локальный базис} --- множество эл.кл., на основе которого базовые алгоритмы строят наборы эл.кл. Использование локального базиса является компромиссом между желанием использовать в логическом корректоре эл.кл. ранга больше 1 и трудоёмкостью вычислений при поиске монотонно корректных наборов эл.кл. Формирование локального базиса само по себе является непростой задачей, и каждый алгоритм решает её по-разному. В нашем случае MONS использует стохастические методы, каждый раз формируя случайно небольшой локальный базис и на его основании составляя новые наборы эл.кл., которые добавляются к растущим семействам.

Для поиска минимальных наборов эл.кл. используется следующий алгоритм. Пусть $LB = \lbrace P_{(H_1, \sigma_1)}, \dots, P_{(H_{N_K}, \sigma_{N_K}),}\rbrace$ -- локальный базис из $N_K$ эл.кл., составленный для некоторого фиксированного класса $K \in \lbrace K_1, ..., K_l \rbrace$. Рассмотрим всевозможные пары объектов вида $(S',S'')$ и сопоставим каждой паре бинарный вектор вида $B(S', S'') = (b_1, \dots, b_{N_K})$, где
\[
b_i(S', S'') = \begin{cases}
1, & \text{если $P_{(H_i, \sigma_i)}(S') = 1$ и $P_{(H_i, \sigma_i)}(S'') = 0$;} \\
0, & \text{иначе.}
\end{cases}
\]

Составим булеву матрицу $L_K$ из всевозможных строк вида $B(S', S'')$, где $S' \in T \cap K$, а $S'' \in T \cap \overline{K}$. Заметим, что каждый столбец $L_K$ соответствует определённому эл.кл. из $LB$, так что набор таких столбцов задаёт набор эл.кл. Очевидно, что любой набор эл.кл. $U_K$ является монотонно корректным тогда и только тогда, когда набор признаков $H = \lbrace H_1, \dots, H_{N_K} \rbrace$ является {\it покрытием матрицы} $L_K$ -- таким набором столбцов, что любая строка на пересечениях с этими столбцами имеет хотя бы одну единицу. Более того, любому неприводимому(минимальному) набору эл.кл однозначно соответствует неприводимое(минимальное) покрытие. Таким образом мы свели задачу к поиску неприводимого покрытия булевой матрицы, которой известен как приложение задачи дуализации \cite{dual_task}. Для решения используются приближённые алгоритмы поиска, такие как градиентный спуск или генетический алгоритм, так как задача дуализации относится к трудно решаемым перечислительным задачам, и полиномиальных алгоритмов для её решения не было открыто до сих пор. 

При обучении алгоритма MONS исходное обучающее множество $T$ (или просто обучающая выборка) разбивается на две части - усечённую обучающую выборку $T'$ и валидационную выборку $V$. Зададим отступ $M(S)$ объекта $S$ как разность между оценками истинного и предсказанного классов. Тогда алгоритм остановится, когда прирост качества для всех объектов станет меньше фиксированного $\varepsilon$.

\begin{algorithm}
\caption{Процедура обучения алгоритма MONS}
\begin{algorithmic}[1]
\REQUIRE $T$ --- обучающая выборка, $max\_i$ --- максимальное число итераций, $\varepsilon$ --- порог;
\ENSURE $W_K, K \in \lbrace K_1, \dots, K_l \rbrace$ --- семейства мон. корректных наборов;
\STATE $V := V(T)$; \COMMENT{случайное выделение выборки $V \subset T$ в качестве валидационной}
\STATE $T' := T \backslash V$; \COMMENT{усечение обучающей выборки}
\FOR{$i=1,\dots, max\_i$}
	\FORALL{$K \in \lbrace K_1, \dots, K_l \rbrace $}
		\STATE $LB := LB()$; \COMMENT{случайным образом сформировать локальный базис}
		\STATE $W_K^i := GA(T', LB, K)$; \COMMENT{ГА возвращает семейство мон. наборов из LB}
		\STATE $W_K := W_K \cap W_K^i$; \COMMENT{добавляем полученные наборы в семейство $W_K$}
	\ENDFOR
	\FORALL {$S \in V$}
		\STATE $M_i(S) := \Gamma_{y(S)}(W^i_{y(S)}, S) - \max\limits_{K \in \lbrace K_1, \dots, K_l \rbrace \backslash y(S)}\Gamma_K(W^i_K, S)  $ \COMMENT{отступ}
		\STATE $M^{avg}_i(S) := \frac{1}{i} \sum_{j = i}^i M_j(S)$	; \COMMENT{средний отступ по предыдущим итерациям}
	\ENDFOR
	\IF {$\forall S \in V M_i^{avg}(S) - M_{i-1}^{avg}(S) < \varepsilon$}
		\STATE {\bf выход}
	\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\newpage

\subsection{Генетический алгоритм}
На каждом шаге генетического алгоритма обновляется множество приближённых решений задачи, называемое {\it популяцией} в ходе итераций, называемых {\it поколениями}. Элементы популяции называются {\it особями}. Качество найденного приближенного решения характеризуется {\it функцией приспособленности}. При развитии популяции можно придерживаться различных подходов, к примеру, стратегия частичной замены подразумевает, что часть популяции переходит в следующее поколение без изменений. При таком подходе популяция постепенно избавляется от <<наименее приспособленных>> особей, но для этого необходимо поддерживать разнообразие особей, иначе генетический алгоритм преждевременно сойдется, попав в локальный минимум. Получение новых особей происходит с помощью операторов скрещивания и мутации, которые имеют явные параллели со своими биологическими аналогами.

В данном случае особями популяции являются покрытия булевой матрицы, кодирующие соответственные наборы эл.кл. Функция приспособленности для набора эл.кл. $U$ задаётся следующей формулой:
\[
\begin{cases}
f(U_i) = \tau(U_i) - \min\limits_{j \in \lbrace 1, \dots, N \rbrace}\tau(U_j) + 1, \\
\tau(U_i) = \frac{1}{|T' \cap K|} \sum_{S \in T' \cap K} \frac{1}{|V \cap K|} \sum_{S' \in V \cap K}\delta_{U_i}(S, S'),
\end{cases}
\]
где $T'$ - усеченная обучающая, а $V$ - валидационная выборки. Функцию $f(U_i)$ требуется максимизировать --- это важно, так как при разных функциях приспособленности могут возникнуть задача как максимизации, так и минимизации.
{\it Начальная популяция} формируется из случайных наборов столбцов, которые, если понадобится, дополняются до покрытий. Если случайно сгенерированная особь уже присутствует в начальной популяции, то ничего не добавляется. {\it Выбор подпопуляции для скрещивания} осуществляется с помощью метода рулетки, когда вероятность выбора зависит от значения функции приспособленности у особи:
\[
p_i = \frac{1/f_i}{\sum^{N_P}_{j=1} 1/f_j}
\]
На каждой итерации для дальнейшего скрещивания выбирается ровно одна пара родителей, так как в этом случае лучшие особи остаются в~популяции и сразу же могут участвовать в следующих скрещиваниях (метод {\it последовательной замены}). Новая особь получается с помощью взвешенного однородного кроссовера, когда значение каждой хромосомы копируется в набор потомка из набора одного из родителей с вероятностью, пропорциональной их значениям функции приспособленности. В алгоритме также используется особый оператор мутации, который повышает вероятность мутации с течением времени по следующей формуле:
\[
k(t) = k_0 \left( 1 - \frac{1}{C \cdot t+1}\right),
\] 
где $k(t)$ - количество мутируемых хромосом на шаге $t$, $k_0$ - количество мутируемых хромосом на последнем шаге алгоритма, коэффициент $C$ отражает скорость стремления $k(t)$ к $k_0$. Затем восстанавливается допустимость решения, так как, вообще говоря, полученный набор столбцов не является неприводимым покрытием. На последнем этапе одна из наименее приспособленных особей замещается потомком.   

\newpage
\section{Сведение задачи восстановления регрессии к задаче классификации}
\subsection{Алгоритм MONS-Reg}
В данной работе был разработана модель {\it MONS-Reg}, которая решает задачу восстановления регрессии и является модификацией стохастического алгоритма классификации MONS. На первом этапе данные разбиваются по признаковому описанию на несколько кластеров с помощью алгоритма кластеризации DM-DBSCAN. На втором этапе применяется стандартный MONS для решения возникшей задачи классификации. На третьем этапе восстанавливается значение целевой переменной по полученной метке класса. Можно исходить из предположения, что у похожих объектов близкие значения целевой переменной, либо не допускать подобного. В первом случае логично разбить объекты(точки в пространстве признаковых описание) по их признаковым описаниям на несколько кластеров, каждому из которых будет присвоена своя метка класса, и сведём задачу к предыдущей. Во втором случае мы можем разбить на классы исходя только из значения самой целевой переменной. 

\subsection{Алгоритм кластеризации DM-DBSCAN}
Для разбиения данных на кластеры можно использовать разные алгоритмы, но важно, чтобы алгоритм умел самостоятельно определять количество кластеров, присутствующих в выборке, отфильтровывал выбросы, умел работать с нелинейной геометрией данных а также был быстрым и простым в реализации. Таким параметрам удовлетворяет алгоритм DM-DBSCAN\cite{dmdbscan} --- динамический плотностной алгоритм кластеризации пространственных данных с присутствием шума. Он, как и его предшественник DBSCAN, основывается на предположении, что внутри каждого кластера наблюдается типичная плотность объектов, которая заметно выше плотности снаружи, в то время как шумовые объекты разрежены сильнее, чем информативные. 

Согласно алгоритму, объекты разбиваются на три типа: <<ядровые>>, <<граничные>> и <<шумовые>>. Для каждой точки в заранее заданном радиусе $\varepsilon$ считается число соседних -- если оно больше некоторого порога, то она находится в середине предполагаемого кластера и называется {\it ядровой}. В таком случае ей присваивается некоторая метка класса, и процесс рекурсивно повторяется для всех её соседей, которые ещё не были обработаны. Если же соседей меньше, чем нужно, но в их числе есть хоть один ядровой объект, то текущий объект является {\it граничным}. После первоначальной разметки(когда обрабатываются только ядровые точки) граничным объектам присваиваются метки ближайших ядровых. В противном случае, если в окрестности слишком мало соседей, а также нет ядровых точек, то объект помечается как {\it шумовой}.
Стоит отметить, что у алгоритма DBSCAN два входных параметра: радиус распознавания соседей $\varepsilon$ и порог для определения ядровых точек $n_{min}$. Но так как плотность получается фиксированной, то DBSCAN не умеет обрабатывать кластеры с различной плотностью. DM-DBSCAN лишён этих недостатков, так как оценивает уровни плотности каждого из кластеров по графику кривой расстояний до k-го ближайшего соседа. 

\begin{figure}[H]
\noindent\centering{
\minipage{0.49\textwidth}
  \includegraphics[width=\linewidth]{graph1}
  \caption{Кривая k-расстояний для одного уровня плотности}\label{fig:graph1}
\endminipage\hfill
\minipage{0.49\textwidth}%
  \includegraphics[width=\linewidth]{graph3}
  \caption{Кривая k-расстояний для нескольких уровней плотности}\label{fig:graph3}
\endminipage
}
\end{figure}
На графиках горизонтальные участки соответствуют уровням плотности в данных, вертикальные участки соответствуют уровням шумовых точек. Для определения оптимальных значений используются точки перемены знака второй производной графика, которая вычисляется по стандартной разностной схеме. 

\newpage
\begin{algorithm}[H]
\caption{Алгоритм DM-DBSCAN}
\begin{algorithmic}[1]
\REQUIRE $T$ --- множество данных из $n$ точек, $n_{min}$ --- минимальное число соседей;
\ENSURE $Y$ --- вектор разметки данных по кластерам;
\STATE $D = dist\_mat(T)$; \COMMENT{матрицы расстояний по выбранной метрике}
\STATE $D^d = k\_deriv(D)$; \COMMENT{вторых производных графика k-расстояний}
\STATE $E = \emptyset, N = \emptyset$; \COMMENT{инициализация множеств порогов плотностей и шумов}
\FOR{$i=1,\dots, n-1$}
	\IF{$D^d_i < 0$ {\bf и} $D^d_{i+1} > 0$ и $D^d_i-D^d_{i+1} < \delta$}
		\STATE  $E = E \cup \lbrace \frac{D^d_i + D^d_{i+1}}{2} \rbrace$ \COMMENT {добавляем в множество порогов плотностей }
	\ELSIF{$D^d_i > 0$ {\bf и} $D^d_{i+1} < 0$ и $D^d_i-D^d_{i+1} < \delta$}
		\STATE $N = N \cup \lbrace \frac{D^d_i + D^d_{i+1}}{2} \rbrace$ \COMMENT {или добавляем в множество порогов шумов}
	\ENDIF
\ENDFOR
\STATE $PQ = \emptyset, C = 0$; \COMMENT{очередь обработки и переменная для меток класса}
\STATE $PD = \emptyset, BD = \emptyset$; \COMMENT{множества обработанных и граничных точек}
\FOR{$i=1, \dots, n$}
	\IF{$type(T_i, E, N) =$ <<ядровая точка>>}		
		\STATE $Y_{T_i} = C$; \COMMENT{присвоим новую метку класса}
		\STATE $PD = PD \cup \lbrace T_i \rbrace$; \COMMENT{помечаем точку как обработанную}
		\STATE $PQ = PQ \cup (neighboors(T_i, E) \backslash PD)$; \COMMENT{добавляем в $PQ$ всех соседей $T_i$} 
		\WHILE {$PQ \neq \emptyset$}
			\IF{$PQ_1 \notin PD$ {\bf и} $type(T_i, E, N) =$ <<ядровая точка>>}
				\STATE $Y_{PQ_1} = C$; \COMMENT{присвоим новую метку класса}
				\STATE $PQ = PQ \cup (neighboors(PQ_1, E) \backslash PD)$; \COMMENT{добавляем соседей $PQ_1$}
			\ELSIF{$PQ_1 \notin PD$ {\bf и} $type(T_i, E, N) =$ <<граничная точка>>}
				\STATE $BD = BD \cup \lbrace T_i \rbrace$;
			\ENDIF
			\STATE $PQ = PQ \backslash \lbrace PQ_1 \rbrace$;
		\ENDWHILE
		\STATE $C = C+1$;
	\ELSIF{$type(T_i, E, N) =$ <<граничная точка>>}
		\STATE $BD = BD \cup \lbrace T_i \rbrace$;
	\ELSIF{$type(T_i, E, N) =$ <<шум>>}
		\STATE $Y_i = -1$; \COMMENT {шум помечается отдельной меткой} 	
	\ENDIF
\ENDFOR
\FOR{$i=1, \dots, |BD|$}
	\STATE $Y_{BD_i} = label(closest\_core(BD_i, D))$;
\ENDFOR
\end{algorithmic}
\end{algorithm}
\newpage

\subsection{Восстановление целевой переменной}
После разбиения на кластеры используется алгоритм MONS для решения уже знакомой задачи классификации, настраивая модель. При распознавании нового объекта алгоритм MONS в качестве ответа выдаст один из кластеров, к которому объект, скорее всего, принадлежит. Найдём значение целевой функции объекта, основываясь на значениях объектов из данного кластера. Есть несколько вариантов поведения, например брать среднее значение или медиану всех значений целевой переменной в классе или у некоторых объектов в нашем классе, которые наиболее похожи на тестируемый --- можно использовать разные метрики, в зависимости от типа данных.

\section{Вычислительные эксперименты}
Алгоритм {\it MONS-Reg} сравнивался с 4 основными моделями, решающими задачу восстановления регрессии:
линейной регрессией(далее обозначаемой как {\it Linear}), случайным лесом ({\it RF}), методом ближайших соседей({\it kNN}) и многослойным персептроном({\it MLP}). Реализация данных моделей была взята из библиотеки машинного обучения scikit-learn. Использовались стандартные параметры. Для тестирования было выбрано 8 прикладных задач из ресурса UCI. Для оценки качества работы применялся скользящий контроль на 10 разбиениях по функционалу качества MSE по следующей формуле:
\[
MSE(y, \hat{y}) = \frac{1}{n} \sum^n_{i=1} (y_i - \hat{y_i})^2,
\]
где $y$ - вектор истинных значений ответов для объектов проверочной выборки, $\hat{y}$ - вектор предсказанных моделью ответов, $n$ - размер выборки.

Ниже в таблице 1 приведены результаты работы перечисленных моделей на соответствующих задачах:
\begin{table}[H]
\caption{Оценка MSE}
\begin{center}
\begin{tabular}{|c|m{15mm}|c|c|c|c|c|}
\hline
Данные & Размер ($m \times n$) & MONS-Reg & Linear & RF & kNN & MLP \\
\hline
Servo              & $167 \times 4$ & 0.95662 & 1.37443 & 0.47613 & 0.94863 & {\bf 0.38500}\\
Computers Hardware & $209 \times 5$ & {\bf 3908.57} & 8128.60 & 7916.62 & 8409.37 & 8869.31\\
Yachts             & $308 \times 6$ & 248.166 & 85.5714 & {\bf 1.58703} & 137.483 & 13.5737\\
Concrete Slump     & $103 \times 7$ & 25.5992 & 8.12311 & 18.6376 & 22.8047 & {\bf 6.92463}\\ 
Breast Cancer      & $198 \times 31$ & 0.12037 & 0.15615 & 0.15713 & {\bf 0.11056} & 0.23432\\
Breast Cancer(wdbc)& $699 \times 8$ & 0.09354 & 0.06418 & {\bf 0.04670} & 0.06032 & 0.16360\\
Breast Cancer(wpbc)& $569 \times 29$ & {\bf 0.18724} & 0.19486 & 0.20308 & 0.20970 & 0.38867\\
Forest Fire        & $517 \times 12$ & 5157.95 & {\bf 4168.22} & 5903.30 & 4746.72 & 21395.5\\
\hline
\end{tabular}
\end{center}
\end{table}

Из таблицы видно, что {\it MONS-Reg} превосходит другие алгоритмы восстановления регрессии на двух задачах из списка и показывает схожее качество на остальных. Из минусов можно отметить большее время работы по сравнению с остальными алгоритмами, так как модель является более сложной и  таблицы видно, что модель способна соперничать с ведущими моделями, а порой и превосходить их на отдельных задачах.

\section{Заключение}
В данной работе была рассмотрена задача восстановления регрессии. Предложен новый подход к решению, основанный на использовании модели логического корректора. Реализованы стохастическая модификация логического корректора алгоритм MONS и динамический плотностной алгоритм кластеризации DM-DBSCAN.
Поставленная задача решена на основе ансамбля алгоритмов. Приведены результаты тестирования алгоритма
на прикладных задачах.

\renewcommand{\bibname}{Список литературы}
\addcontentsline{toc}{section}{\bibname}

\nocite{hastie09elements,bishop06pattern,zhuravlev06recognition,zhuravlev78prob33,shlezinger04ten,boucheron05theory}

\def\BibUrl#1.{}\def\BibAnnote#1.{}
%\def\BibUrl#1{\\{\footnotesize\tt\def~{\char126} http://#1}}
\bibliographystyle{gost71s}
\bibliography{MachLearn}
\begin{thebibliography}{0}
\bibitem{new_pprok} {\it Дюкова Е.В., Журавлёв Ю.И., Прокофьев П.А.} Логические корректоры в задаче классификации по прецедентам // Ж. вычисл. матем. и матем. физики, 2017, 57:11 С.141-162

\bibitem{sotnezov11} {\it Djukova E.V., Zhuravlev Yu.I., Sotnezov R.M.} Construction of an Ensemble of Logical Correctors on the Basis of Elementary Classifiers // Pattern Recognition and Image Analysis, 2011, Vol. 21, No4, pp. 599—605.

\bibitem{alg_meth} {\it Журавлёв Ю.И.} Об алгебраическом подходе к решению задач распознавания или классификации // Проблемы кибернетики, 1978, вып. 33, С.5–68

\bibitem{main_source}{\it Дюкова Е.В., Журавлёв Ю.И., Рудаков К.В.} Об алгебро-логическом синтезе корректных процедур распознавания на базе элементарных алгоритмов // Ж. вычисл. матем. и матем. физики, 1996, 36:8   215–223

\bibitem{sotnezov09} {\it Sotnezov R.M.} Genetic Algorithms for Problems of Logical Data Analysis in Discrete Optimization and Image Recognition // Pattern Recognition and Image Analysis, 2009, Vol. 19, No. 3, 469–477

\bibitem{lyubimtseva14} {\it Любимцева М.М.} Логические корректоры в задачах распознавания// Тезисы лучших дипломных работ факультета ВМК МГУ, 2014. С.47-49.

\bibitem{dmdbscan} {\it Ashour W.M., Elbatta M.T.H.} A Dynamic Method for Discovering Density Varied Clusters // International Journal of Signal Processing, Image Processing and Pattern Recognition Vol.6, No. 1, 2013 

\bibitem{dual_task} {\it Дюкова Е.В.} Об асимптотически оптимальном алгоритме построения тупиковых тестов // ДАН СССР. 1977. Т. 233. № 4. С. 527-530. 

\end{thebibliography}

\end{document}
