\documentclass[12pt,fleqn]{article}
\usepackage[cp1251]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage{amsfonts,amssymb,amsmath,mathrsfs,amsthm}
\usepackage[russian]{babel}
\usepackage{graphicx}
\usepackage[footnotesize]{caption2}
\usepackage{indentfirst}
%\usepackage[ruled,section]{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{algorithm}
%\usepackage[all]{xy}

% Параметры страницы
\textheight=24cm
\textwidth=16cm
\oddsidemargin=5mm
\evensidemargin=-5mm
\marginparwidth=36pt
\topmargin=-1cm
\footnotesep=3ex
%\flushbottom
\raggedbottom
\tolerance 3000
% подавить эффект "висячих стpок"
\clubpenalty=10000
\widowpenalty=10000
\renewcommand{\baselinestretch}{1.1}
\renewcommand{\baselinestretch}{1.5} %для печати с большим интервалом


\def\algorithmicrequire{\textbf{Вход:}}
\def\algorithmicensure{\textbf{Выход:}}
\def\algorithmicif{\textbf{если}}
\def\algorithmicthen{\textbf{то}}
\def\algorithmicelse{\textbf{иначе}}
\def\algorithmicelsif{\textbf{иначе если}}
\def\algorithmicfor{\textbf{для}}
\def\algorithmicforall{\textbf{для всех}}
\def\algorithmicdo{}
\def\algorithmicwhile{\textbf{пока}}
\def\algorithmicrepeat{\textbf{повторять}}
\def\algorithmicuntil{\textbf{пока}}
\def\algorithmicloop{\textbf{цикл}}
% переопределение стиля комментариев
\def\algorithmiccomment#1{\quad// {\sl #1}}


\begin{document}

\begin{titlepage}
\begin{center}
    Московский государственный университет имени М. В. Ломоносова

    \bigskip
    \includegraphics[width=50mm]{msu.eps}

    \bigskip
    Факультет Вычислительной Математики и Кибернетики\\
    Кафедра Математических Методов Прогнозирования\\[10mm]

    \textsf{\large\bfseries
        КУРСОВАЯ РАБОТА СТУДЕНТА 317 ГРУППЫ\\[7mm]
        \normalsize << Логические корректоры в задаче восстановления регрессии >> 
    }\\[13mm]

    \begin{flushright}
        \parbox{0.5\textwidth}{
            Выполнил:\\
            студент 3 курса 317 группы\\
            \emph{Байтеков Никита Вячеславович}\\[5mm]
            Научный руководитель:\\
            д.ф-м.н., доцент\\
            \emph{Дюкова Елена Всеволодовна}
        }
    \end{flushright}

    \vspace{\fill}
    Москва, 2017
\end{center}
\end{titlepage}

\newpage
\renewcommand{\contentsname}{Содержание}
\tableofcontents

\newpage

\section{Введение} 
Рассмотрим множество объектов $M$, которые описываются набором целочисленных признаков $\lbrace x_1, ..., x_n \rbrace$. Каждому объекту соответствует значение из множества ответов $Y$. Дано обучающее множество объектов $ T = \lbrace S_1, ..., S_m \rbrace $ из $M$, такое, что для каждого обучающего объекта(прецедента) известно значение ответа. Требуется построить алгоритм распознавания $A_T : M \to Y$, ставящий в соответствие произвольному объекту из $M$ ответ из $Y$. В случае, когда $Y = \mathbb{R}$, решается задача восстановления регрессии, а при $Y = \lbrace 0, 1, \dots , l \rbrace$ --- задача классификации.

В работе рассматривается задача восстановления регрессии, для решения которой существует много методов. Чаще всего используется многомерная линейная регрессия, когда зависимость ответа от признаков предполагается линейной. Хорошо показывает себя метод ближайших соседей, который исходит из предположения, что у похожих объектов будут похожие ответы. Для решения задачи также используются адаптации методов классификации. В качестве примера можно рассмотреть вариации решающих деревьев, которые стабильно показывают высокое качество полученных решений.

Одной из современных моделей классификации является логический корректор \cite{sotnezov11}, в основе которого лежат идеи как логического(использование семейств простых логических моделей), так и алгебраического подходов(взаимное корректирование ошибок различных алгоритмов)\cite{alg_meth}. Метод хорошо зарекомендовал себя при решении прикладных задач. В \cite{lyubimtseva14} разработан стохастический вариант логического корректора MONS. Целью данной работы является адаптация этого алгоритма к задаче восстановления регрессии.

Введём основные определения. Задано обучающее множество $T$, и для каждого прецедента из $T$ известно, к какому из $l$ непересекающихся множеств $\lbrace K_1, ..., K_l \rbrace$, называемых {\it классами}, он принадлежит. Пусть $T \cap K$ --- множество прецедентов из $T$, принадлежащих классу $K$, а $T \cap \overline{K}$ --- множество прецедентов из $T$, не принадлежащих классу $K$. Конъюнкция вида $P_{(H, \sigma)} = x^{\sigma_1}_{j_1}...x^{\sigma_r}_{j_r}$ называется {\it элементарным классификатором(эл.кл.)}, где $H = \lbrace x_{j_1}, ..., x_{j_r} \rbrace$ --- набор целочисленных признаков, $\sigma = \lbrace \sigma_1, ..., \sigma_r \rbrace$ --- набор допустимых значений этих признаков, а $r$ --- {\it ранг эл.кл.}. Эл.кл. считается {\it корректным для класса} $K$, если $\nexists (S', S'')$ — пары обучающих объектов, $S' \in K, S'' \notin K$, такой, что $ P_{(H, \sigma)}(S') = P_{(H, \sigma)}(S'') = 1 $.
Объект $S$ {\it содержит эл.кл.} $P_{(H, \sigma)}$, если $P_{(H, \sigma)}(S) = 1$.
Пусть $U = \lbrace P_{x_{j_1}, \sigma_{j_1}}, ..., P_{x_{j_q}, \sigma_{j_q}} \rbrace$ - {\it набор эл.кл.}. Набор эл.кл. $U$ называется {\it монотонно корректным для класса} $K$, где $K \in \lbrace K_1, ..., K_l \rbrace$, если $\forall (S', S'')$ — пары обучающих объектов, $S' \in K, S'' \notin K$ существует монотонная функция алгебры логики $F_{U,K}$: $F_{U,K}(U(S')) \neq F_{U,K} (U(S''))$. В таком случае, функция $F_{U,K}$ называется {\it монотонно корректирующей} для $U$. Монотонно корректный набор $U$ для класса $K$ является {\it неприводимым}, если любой набор эл.кл., являющийся подмножеством $U$, не является монотонно корректным. Неприводимый набор для класса $K$ является {\it минимальным}, если не существует монотонно корректных наборов для класса $K$ меньшей мощности.

\section{Логические корректоры в задаче классификации}


\subsection{Алгоритм MON}
Одним из первых логических корректоров, показывающих хорошие результаты, является алгоритм MON \cite{sotnezov09}, который использовал монотонно корректные наборы, состоящие из эл.кл. ранга 1.

Процесс работы корректора MON разбивается на два этапа: обучение модели и распознавание. На этапе обучения для каждого класса $K$, $K \in \lbrace K_1, ..., K_l \rbrace$, распознающий алгоритм $A$ строит семейство монотонно корректных наборов эл.кл. $W_A(K)$. При распознавании для каждого объекта $S$ вычисляется оценка принадлежности этого объекта к каждому из классов $K$ по формуле:
\[
\Gamma_K (S) = \frac{1}{|W_A(K)|} \sum_{U \in W_A(K)} \frac{1}{|T \cap K|} \sum_{S' \in T \cap K} \delta_U(S, S').
\]
где $\delta_U(S, S')$ называется {\it функцией голосования} и определяется как:
\[
\delta_U(S, S') = \begin{cases}
1, & \text{если $\omega_U(S) \succeq \omega_U(S')$;} \\
0, & \text{иначе.}
\end{cases}
\]
Алгоритм относит неизвестный объект S к тому классу, для которого оценка принадлежности будет максимальной.

\subsection{Алгоритм MONS}
Алгоритм MONS является улучшенной моделью алгоритма MON, который использует т.н. {\it локальный базис} --- множество эл.кл., на основе которого базовые алгоритмы строят наборы эл.кл. Использование локального базиса является компромиссом между желанием использовать в логическом корректоре эл.кл. ранга больше 1 и трудоёмкостью вычислений при поиске монотонно корректных наборов эл.кл. Формирование локального базиса само по себе является непростой задачей, и каждый алгоритм решает её по-разному. В нашем случае MONS использует стохастические методы, каждый раз формируя случайно небольшой локальный базис и на его основании составляя новые наборы эл.кл., которые добавляются к растущим семействам.

Для поиска минимальных наборов эл.кл. используется следующий алгоритм. Пусть $LB = \lbrace P_{(H_1, \sigma_1)}, \dots, P_{(H_{N_K}, \sigma_{N_K}),}\rbrace$ -- локальный базис из $N_K$ эл.кл., составленный для некоторого фиксированного класса $K \in \lbrace K_1, ..., K_l \rbrace$. Рассмотрим всевозможные пары объектов вида $(S',S'')$ и сопоставим каждой паре бинарный вектор вида $B(S', S'') = (b_1, \dots, b_{N_K})$, где:
\[
b_i(S', S'') = \begin{cases}
1, & \text{если $P_{(H_i, \sigma_i)}(S') = 1$ и $P_{(H_i, \sigma_i)}(S'') = 0$;} \\
0, & \text{иначе.}
\end{cases}
\]

Составим булеву матрицу $L_K$ из всевозможных строк вида $B(S', S'')$, где $S' \in T \cap K$, а $S'' \in T \cap \overline{K}$. Заметим, что каждый столбец $L_K$ соответствует определённому эл.кл. из $LB$, так что набор таких столбцов задаёт набор эл.кл. Очевидно, что любой набор эл.кл. $U_K$ является монотонно корректным тогда и только тогда, когда набор признаков $H = \lbrace H_1, \dots, H_{N_K} \rbrace$ является {\it покрытием матрицы} $L_K$ -- таким набором столбцов, что любая строка на пересечениях с этими столбцами имеет хотя бы одну единицу. Более того, любому неприводимому(минимальному) набору эл.кл однозначно соответствует неприводимое(минимальное) покрытие. Таким образом мы свели задачу к поиску неприводимого покрытия булевой матрицы, которой известен как приложение задачи дуализации \cite{dual_task}. Для решения используются приближённые алгоритмы поиска, такие как градиентный спуск или генетический алгоритм, так как задача дуализации относится к трудно решаемым перечислительным задачам, и полиномиальных алгоритмов для её решения не было открыто до сих пор. 

Градиентный алгоритм хорошо показывает себя на случайных булевых матрицах, но сильно ошибается на <<разряженных>> матрицах, которые содержат относительно малое количество единиц. Этого недостатка лишён генетический алгоритм, который и используется в данной работе.

При обучении алгоритма MONS исходное обучающее множество $T$ (или просто обучающая выборка) разбивается на две части - усечённую обучающую выборку $T'$ и валидационную выборку $V$. После выше описанного алгоритма для получения минимальных наборов эл.кл. для каждого класса требуется проверить критерий останова, ведь найдено лишь приближённое решение, а первой итерации может не хватить. Зададим отступ объекта $S$ как разность между оценками истинного и предсказанного классов. Тогда алгоритм остановится, когда прирост качества для всех объектов станет меньше фиксированного $\varepsilon$:

\begin{algorithm}
\caption{Процедура обучения алгоритма MONS}
\begin{algorithmic}[1]
\REQUIRE $T$ --- обучающая выборка, $max\_i$ --- максимальное число итераций, $\varepsilon$ --- порог;
\ENSURE $W_K, K \in \lbrace K_1, \dots, K_l \rbrace$ --- семейства мон. корректных наборов;
\STATE $V := V(T)$; \COMMENT{случайное выделение выборки $V \subset T$ в качестве валидационной}
\STATE $T' := T \backslash V$; \COMMENT{усечение обучающей выборки}
\FOR{$i=1,\dots, max\_i$}
	\FORALL{$K \in \lbrace K_1, \dots, K_l \rbrace $}
		\STATE $LB := LB()$; \COMMENT{случайным образом сформировать локальный базис}
		\STATE $W_K^i := GA(T', LB, K)$; \COMMENT{ГА возвращает семейство мон. наборов из LB}
		\STATE $W_K := W_K \cap W_K^i$; \COMMENT{добавляем полученные наборы в семейство $W_K$}
	\ENDFOR
	\FORALL {$S \in V$}
		\STATE $M_i(S) := \Gamma_{y(S)}(W^i_{y(S)}, S) - \max\limits_{K \in \lbrace K_1, \dots, K_l \rbrace \backslash y(S)}\Gamma_K(W^i_K, S)  $ \COMMENT{отступ}
		\STATE $M^{avg}_i(S) := \frac{1}{i} \sum_{j = i}^i M_j(S)$	; \COMMENT{средний отступ по предыдущим итерациям}
	\ENDFOR
	\IF {$\forall S \in V M_i^{avg}(S) - M_{i-1}^{avg}(S) < \varepsilon$}
		\STATE {\bf выход}
	\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

\newpage

\subsection{Генетический алгоритм}
На каждом шаге генетического алгоритма обновляется множество приближённых решений задачи, называемое {\it популяцией} в ходе итераций, называемых {\it поколениями}. Элементы популяции также называются особями. Качество найденного приближенного решения характеризуется {\it функцией приспособленности}. При развитии популяции можно придерживаться различных подходов, к примеру, стратегия частичной замены подразумевает, что часть популяции переходит в следующее поколение без изменений. При таком подходе популяция постепенно избавляется от <<наименее приспособленных>> особей, но для этого необходимо поддерживать разнообразие особей, иначе генетический алгоритм преждевременно сойдется, попав в локальный минимум. Получение новых особей происходит с помощью операторов скрещивания и мутации, которые имеют явные параллели со своими биологическими аналогами.

В данном случае особями популяции являются покрытия булевой матрицы, кодирующие соответственные наборы эл.кл. Функция приспособленности для набора эл.кл. $U$ задаётся следующей формулой:
\[
\begin{cases}
f(U_i) = \tau(U_i) - \min\limits_{j \in \lbrace 1, \dots, N \rbrace}\tau(U_j) + 1, \\
\tau(U_i) = \frac{1}{|T' \cap K|} \sum_{S \in T' \cap K} \frac{1}{|V \cap K|} \sum_{S' \in V \cap K}\delta_{U_i}(S, S'),
\end{cases}
\]
где $T'$ - усеченная обучающая, а $V$ - валидационная выборки. Функцию $f(U_i)$ требуется максимизировать --- это важно, так как при разных функциях приспособленности могут возникнуть задача как максимизации, так и минимизации.
{\it Начальная популяция} формируется из случайных наборов столбцов, которые, если понадобится, дополняются до покрытий. Если случайно сгенерированная особь уже присутствует в начальной популяции, то ничего не добавляется. {\it Выбор подпопуляции для скрещивания} осуществляется с помощью метода рулетки, когда вероятность выбора зависит от значения функции приспособленности у особи:
\[
p_i = \frac{1/f_i}{\sum^{N_P}_{j=1} 1/f_j}
\]
Такая формула позволяет вычислять относительное значение приспособленности вместо абсолютного, предотвращая проблему равных вероятностей выбора особей поздних популяций.
На каждой итерации для дальнейшего скрещивания выбирается ровно одна пара родителей, так как в этом случае лучшие особи остаются в~популяции и сразу же могут участвовать в следующих скрещиваниях (метод {\it последовательной замены}). Новая особь получается с помощью взвешенного однородного кроссовера, когда значение каждой хромосомы копируется в набор потомка из набора одного из родителей с вероятностью, пропорциональной их значениям функции приспособленности. В алгоритме также используется особый оператор мутации, который повышает вероятность мутации с течением времени по следующей формуле:
\[
k(t) = k_0 \left( 1 - \frac{1}{C \cdot t+1}\right),
\] 
где $k(t)$ - количество мутируемых хромосом на шаге $t$, $k_0$ - количество мутируемых хромосом на последнем шаге алгоритма, коэффициент $C$ отражает скорость стремления $k(t)$ к $k_0$. После чего замещаем потомком одну из наименее приспособленных особей, попутно восстанавливая допустимость решения, так как после операторов скрещивания и мутации полученный набор столбцов может не являться неприводимым покрытием или покрытием в принципе. В качестве критерия останова используется {\it сходимость популяции с задержкой}: если прирост качества популяции меньше заранее заданного порога $\varepsilon$ определенное число $t_{tc}$ шагов подряд, то выполнение алгоритма прерывается. Это понижает вероятность преждевременной сходимости, так как у алгоритма больше шансов, мутировав, <<выбраться>> из локального минимума.     

\newpage
\section{Сведение задачи восстановления регрессии к задаче классификации}
Для применения алгоритма MONS в задаче восстановления регрессии предлагается адаптировать нашу задачу к задаче классификации. Мы можем либо исходить из предположения, что у похожих объектов близкие значения целевой переменной, либо не допускать подобного. В первом случае логично разбить объекты(точки в пространстве признаковых описание) по их признаковым описаниям на несколько кластеров, каждому из которых будет присвоена своя метка класса, и сведём задачу к предыдущей. Во втором случае мы можем разбить на классы исходя только из значения самой целевой переменной. 

\subsection{Алгоритм DM-DBSCAN}
Для разбиения данных на кластеры можно использовать разные алгоритмы, но важно, чтобы алгоритм умел самостоятельно определять количество кластеров, присутствующих в выборке, отфильтровывал выбросы, умел работать с нелинейной геометрией данных а также был быстрым и простым в реализации. Таким параметрам удовлетворяет алгоритм DM-DBSCAN\cite{dmdbscan} --- динамический плотностной алгоритм кластеризации пространственных данных с присутствием шума. Он, как и его предшественник DBSCAN, основывается на предположении, что внутри каждого кластера наблюдается типичная плотность объектов, которая заметно выше плотности снаружи, в то время как шумовые объекты разрежены сильнее, чем информативные. 

Согласно алгоритму, объекты разбиваются на три типа: <<ядровые>>, <<граничные>> и <<шумовые>>. Для каждой точки в заранее заданном радиусе $\varepsilon$ считается число соседних -- если оно больше некоторого порога, то она находится в середине предполагаемого кластера и называется {\it ядровой}. В таком случае ей присваивается некоторая метка класса, и процесс рекурсивно повторяется для всех её соседей, которые ещё не были обработаны. Если же соседей меньше, чем нужно, но в их числе есть хоть один ядровой объект, то текущий объект является {\it граничным}. После первоначальной разметки(когда обрабатываются только ядровые точки) граничным объектам присваиваются метки ближайших ядровых. В противном случае, если в окрестности слишком мало соседей, а также нет ядровых точек, то объект помечается как {\it шумовой}.
Стоит отметить, что у алгоритма DBSCAN два входных параметра: радиус распознавания соседей $\varepsilon$ и порог для определения ядровых точек $n_{min}$. Но так как плотность получается фиксированной, то DBSCAN не умеет обрабатывать кластеры с различной плотностью. DM-DBSCAN лишён этих недостатков, так как оценивает уровни плотности каждого из кластеров по графику кривой расстояний до k-го ближайшего соседа. 

\begin{figure}[H]
\noindent\centering{
\minipage{0.49\textwidth}
  \includegraphics[width=\linewidth]{graph1}
  \caption{Кривая k-расстояний для одного уровня плотности}\label{fig:graph1}
\endminipage\hfill
\minipage{0.49\textwidth}%
  \includegraphics[width=\linewidth]{graph3}
  \caption{Кривая k-расстояний для нескольких уровней плотности}\label{fig:graph3}
\endminipage
}
\end{figure}
На графиках можно наблюдать горизонтальные участки, соответствующие уровням плотности в данных, в то время как вертикальные участки соответствуют уровням шумовых точек. Для определения оптимальных значений используются точки перемены знака второй производной графика, которая вычисляется по стандартной разностной схеме. 

После разбиения на кластеры используется алгоритм MONS для решения уже знакомой задачи классификации, настраивая модель. При распознавании нового объекта алгоритм MONS в качестве ответа выдаст один из кластеров, к которому объект, скорее всего, принадлежит. Найдём значение целевой функции объекта, основываясь на значениях объектов из данного кластера. Есть несколько вариантов поведения, например брать среднее значение или медиану всех значений целевой переменной в классе или у некоторых объектов в нашем классе, которые наиболее похожи на тестируемый --- можно использовать разные метрики, в зависимости от типа данных.

\newpage

\begin{algorithm}[H]
\caption{Алгоритм DM-DBSCAN}
\begin{algorithmic}[1]
\REQUIRE $T$ --- множество данных из $n$ точек, $n_{min}$ --- минимальное число соседей;
\ENSURE $Y$ --- вектор разметки данных по кластерам;
\STATE $D = dist\_mat(T)$; \COMMENT{матрицы расстояний по выбранной метрике}
\STATE $D^d = k\_deriv(D)$; \COMMENT{вторых производных графика k-расстояний}
\STATE $E = \emptyset, N = \emptyset$; \COMMENT{инициализация множеств порогов плотностей и шумов}
\FOR{$i=1,\dots, n-1$}
	\IF{$D^d_i < 0$ {\bf и} $D^d_{i+1} > 0$ и $D^d_i-D^d_{i+1} < \delta$}
		\STATE  $E = E \cup \lbrace \frac{D^d_i + D^d_{i+1}}{2} \rbrace$ \COMMENT {добавляем в множество порогов плотностей }
	\ELSIF{$D^d_i > 0$ {\bf и} $D^d_{i+1} < 0$ и $D^d_i-D^d_{i+1} < \delta$}
		\STATE $N = N \cup \lbrace \frac{D^d_i + D^d_{i+1}}{2} \rbrace$ \COMMENT {или добавляем в множество порогов шумов}
	\ENDIF
\ENDFOR
\STATE $PQ = \emptyset, C = 0$; \COMMENT{очередь обработки и переменная для меток класса}
\STATE $PD = \emptyset, BD = \emptyset$; \COMMENT{множества обработанных и граничных точек}
\FOR{$i=1, \dots, n$}
	\IF{$type(T_i, E, N) =$ <<ядровая точка>>}		
		\STATE $Y_{T_i} = C$; \COMMENT{присвоим новую метку класса}
		\STATE $PD = PD \cup \lbrace T_i \rbrace$; \COMMENT{помечаем точку как обработанную}
		\STATE $PQ = PQ \cup (neighboors(T_i, E) \backslash PD)$; \COMMENT{добавляем в $PQ$ всех соседей $T_i$} 
		\WHILE {$PQ \neq \emptyset$}
			\IF{$PQ_1 \notin PD$ {\bf и} $type(T_i, E, N) =$ <<ядровая точка>>}
				\STATE $Y_{PQ_1} = C$; \COMMENT{присвоим новую метку класса}
				\STATE $PQ = PQ \cup (neighboors(PQ_1, E) \backslash PD)$; \COMMENT{добавляем соседей $PQ_1$}
			\ELSIF{$PQ_1 \notin PD$ {\bf и} $type(T_i, E, N) =$ <<граничная точка>>}
				\STATE $BD = BD \cup \lbrace T_i \rbrace$;
			\ENDIF
			\STATE $PQ = PQ \backslash \lbrace PQ_1 \rbrace$;
		\ENDWHILE
		\STATE $C = C+1$;
	\ELSIF{$type(T_i, E, N) =$ <<граничная точка>>}
		\STATE $BD = BD \cup \lbrace T_i \rbrace$;
	\ELSIF{$type(T_i, E, N) =$ <<шум>>}
		\STATE $Y_i = -1$; \COMMENT {шум помечается отдельной меткой} 	
	\ENDIF
\ENDFOR
\FOR{$i=1, \dots, |BD|$}
	\STATE $Y_{BD_i} = label(closest\_core(BD_i, D))$;
\ENDFOR
\end{algorithmic}
\end{algorithm} 
\newpage

\section{Вычислительные эксперименты}

%Цель данного раздела:
%продемонстрировать, что предложенная теория работает на практике;
%показать границы её применимости;
%рассказать о~новых экспериментальных фактах.

%Чисто теоретические работы могут вообще не~содержать раздела экспериментов
%(не~работает, ну и~не~надо~--- зато теория красивая).
%Кстати, теоретики имеют право не~догадываться, где, кому и~когда их теории пригодятся.

\subsection{Исходные данные и~условия эксперимента}
%Описывается прикладная задача, параметры анализируемых данных 
%(например, сколько объектов, сколько признаков, каких они типов), 
%параметры эксперимента 
%(например, как производился скользящий контроль).  

\subsection{Результаты эксперимента}
%Результаты экспериментов представляются в~виде таблиц и~графиков. 
%Объясняется точный смысл всех обозначений на графиках, строк и~столбцов в~таблицах. 

\subsection{Выводы}
%Приводятся выводы: 
%в~какой степени результаты экспериментов согласуются с~теорией? 
%Достигнут ли желаемый результат? 
%Обнаружены ли какие-либо факты, не~нашедшие объяснения, и~которые нельзя списать на «грязный» эксперимент?

%Обсуждаются основные отличия предложенных методов от известных ранее. 
%В~чем их преимущества? 
%Каковы границы их применимости? 
%Какие проблемы удалось решить, а~какие остались открытыми? 
%Какие возникли новые постановки задач?

\section{Заключение}
В данной работе была рассмотрена задача восстановления регрессии. Предложен новый подход к решению, основанный на использовании модели логического корректора. Реализованы стохастическая модификация логического корректора алгоритм MONS и динамический плотностной алгоритм кластеризации DM-DBSCAN.
Поставленная задача решена на основе ансамбля алгоритмов. Приведены результаты тестирования алгоритма
на прикладных задачах.

\renewcommand{\bibname}{Список литературы}
\addcontentsline{toc}{section}{\bibname}

\nocite{hastie09elements,bishop06pattern,zhuravlev06recognition,zhuravlev78prob33,shlezinger04ten,boucheron05theory}

\def\BibUrl#1.{}\def\BibAnnote#1.{}
%\def\BibUrl#1{\\{\footnotesize\tt\def~{\char126} http://#1}}
\bibliographystyle{gost71s}
\bibliography{MachLearn}
\begin{thebibliography}{0}
\bibitem{sotnezov11} {\it E.V. Djukova, Yu.I. Zhuravlev, R.M. Sotnezov.} Construction of an Ensemble of Logical Correctors on the Basis of Elementary Classifiers // Pattern Recognition and Image Analysis, 2011, Vol. 21, No4, pp. 599—605.

\bibitem{alg_meth} {\it Ю. И. Журавлёв} Об алгебраическом подходе к решению задач распознавания или классификации // Проблемы кибернетики, 1978, вып. 33, 5–68

\bibitem{main_source}{\it Е. В. Дюкова, Ю. И. Журавлёв, К. В. Рудаков} Об алгебро-логическом синтезе корректных процедур распознавания на базе элементарных алгоритмов // Журнал вычислительной математики и математической физики, 1996, 36:8   215–223

\bibitem{sotnezov09} {\it R. M. Sotnezov.} Genetic Algorithms for Problems of Logical Data Analysis in Discrete Optimization and Image Recognition // Pattern Recognition and Image Analysis, 2009, Vol. 19, No. 3, 469–477

\bibitem{lyubimtseva14} {\it Любимцева М.М., Дюкова Е.В.} Логические корректоры в задачах распознавания // 2014

\bibitem{dmdbscan} {\it M.T.H. Elbatta, W.M. Ashour} A Dynamic Method for Discovering Density Varied Clusters // International Journal of Signal Processing, Image Processing and Pattern Recognition Vol.6, No. 1, 2013 

\bibitem{dual_task} {\it Дюкова Е.В.} Об асимптотически оптимальном алгоритме построения тупиковых тестов // ДАН СССР. 1977. Т. 233. № 4. С. 527-530. 

\end{thebibliography}

\end{document}
