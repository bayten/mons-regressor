\documentclass[12pt,fleqn]{article}
\usepackage[cp1251]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage{amsfonts,amssymb,amsmath,mathrsfs,amsthm}
\usepackage[russian]{babel}
\usepackage{graphicx}
\usepackage[footnotesize]{caption2}
\usepackage{indentfirst}
%\usepackage[ruled,section]{algorithm}
\usepackage[noend]{algorithmic}
\usepackage{algorithm}
%\usepackage[all]{xy}

% Параметры страницы
\textheight=24cm
\textwidth=16cm
\oddsidemargin=5mm
\evensidemargin=-5mm
\marginparwidth=36pt
\topmargin=-1cm
\footnotesep=3ex
%\flushbottom
\raggedbottom
\tolerance 3000
% подавить эффект "висячих стpок"
\clubpenalty=10000
\widowpenalty=10000
\renewcommand{\baselinestretch}{1.1}
\renewcommand{\baselinestretch}{1.5} %для печати с большим интервалом


\def\algorithmicrequire{\textbf{Вход:}}
\def\algorithmicensure{\textbf{Выход:}}
\def\algorithmicif{\textbf{если}}
\def\algorithmicthen{\textbf{то}}
\def\algorithmicelse{\textbf{иначе}}
\def\algorithmicelsif{\textbf{иначе если}}
\def\algorithmicfor{\textbf{для}}
\def\algorithmicforall{\textbf{для всех}}
\def\algorithmicdo{}
\def\algorithmicwhile{\textbf{пока}}
\def\algorithmicrepeat{\textbf{повторять}}
\def\algorithmicuntil{\textbf{пока}}
\def\algorithmicloop{\textbf{цикл}}
% переопределение стиля комментариев
\def\algorithmiccomment#1{\quad// {\sl #1}}


\begin{document}

\begin{titlepage}
\begin{center}
    Московский государственный университет имени М. В. Ломоносова

    \bigskip
    \includegraphics[width=50mm]{msu.eps}

    \bigskip
    Факультет Вычислительной Математики и Кибернетики\\
    Кафедра Математических Методов Прогнозирования\\[10mm]

    \textsf{\large\bfseries
        КУРСОВАЯ РАБОТА СТУДЕНТА 317 ГРУППЫ\\[7mm]
        \normalsize << Логические корректоры в задаче восстановления регрессии >> 
    }\\[13mm]

    \begin{flushright}
        \parbox{0.5\textwidth}{
            Выполнил:\\
            студент 3 курса 317 группы\\
            \emph{Байтеков Никита Вячеславович}\\[5mm]
            Научный руководитель:\\
            д.ф-м.н., доцент\\
            \emph{Дюкова Елена Всеволодовна}
        }
    \end{flushright}

    \vspace{\fill}
    Москва, 2017
\end{center}
\end{titlepage}

\newpage
\renewcommand{\contentsname}{Содержание}
\tableofcontents

\newpage

\section{Введение} 
Рассмотрим множество объектов $M$, которые описываются набором целочисленных признаков $\lbrace x_1, ..., x_n \rbrace$. Каждому объекту соответствует значение из множества ответов $Y$: если $Y = \mathbb{R}$, то решается задача восстановления регрессии, а если $|Y| < \infty$ --- задача классификации. Также задано обучающее множество объектов $ T = \lbrace S_1, ..., S_m \rbrace $ из $M$, такое, что для каждого обучающего объекта(прецедента) известно значение ответа. Требуется построить алгоритм $A_T : M \to Y$, ставящий в соответствие произвольному объекту из $M$ ответ из $Y$. Алгоритм $A_T$ в таком случае называется алгоритмом (процедурой) распознавания.

В работе рассматривается задача восстановления регрессии, для решения которой существует много методов. Чаще всего используется многомерная линейная регрессия, когда зависимость значения ответа от значений признаков предполагается линейной. Хорошо показывает себя метод ближайших соседей, который исходит из предположения, что у похожих объектов будут похожие ответы. Для решения задачи восстановления регрессии используются также адаптации методов, которые изначально решали задачу классификации, так как исторически сложилось, что таких алгоритмов больше, чем тех, что решают задачу восстановления регрессии. В качестве примера адаптированных методов можно рассмотреть всевозможные вариации решающих деревьев\cite{trees}, которые стабильно показывают высокое качество полученных решений.

При решении прикладных задач классификации хорошо себя зарекомендовали методы, основанные на логическом (комбинаторном) анализе признаковых описаний объектов. В этих методах большое внимание уделяется вопросам синтеза корректных распознающих алгоритмов, т.е. алгоритмов, которые безошибочно классифицируют обучающие объекты. Целью данной работы является адаптация одного из таких алгоритмов к задаче восстановления регрессии, а именно алгоритма MONS --- стохастической разновидности логического корректора\cite{lyubimtseva14}.

\newpage
Пусть $T \cap K$ --- множество прецедентов из $T$ из класса $K$, а $T \cap \overline{K}$ --- множество прецедентов из $T$ не из класса $K$. Конъюнкция вида $B_{(H, \sigma)} = x^{\sigma_1}_{j_1}...x^{\sigma_r}_{j_r}$ называется {\it элементарным классификатором(эл.кл.)}, где $H = \lbrace x_{j_1}, ..., x_{j_r} \rbrace$ --- набор целочисленных признаков, $\sigma = \lbrace \sigma_1, ..., \sigma_r \rbrace$ --- набор допустимых значений этих признаков, а $r$ --- ранг эл.кл. Эл.кл. считается {\it корректным}, если
$\nexists S' \in T \cap K, S'' \in T \cap \overline{K}: B_{(H, \sigma)}(S') = B_{(H, \sigma)}(S'') = 1$.
Объект $S$ {\it содержит эл.кл.} $B_{(H, \sigma)}$, если $B_{(H, \sigma)}(S) = 1$.
Пусть $U = \lbrace B_{x_{j_1}, \sigma_{j_1}}, ..., B_{x_{j_q}, \sigma_{j_q}} \rbrace$ - {\it набор элементарных классификаторов}. Набор эл.кл. $U$ называется {\it(монотонным) корректным для класса} $K$, где $K \in \lbrace K_1, ..., K_l \rbrace$, если $\forall (S', S'')$ — пары обучающих объектов, $S' \in K, S'' \notin K$ существует (монотонная) функция алгебры логики $F_{U,K}$: $F_{U,K}(U(S')) \neq F_{U,K} (U(S''))$. Функция алгебры логики $F(t_1, ..., t_d)$, такая, что для любых двух обучающих объектов $S_i \in K$ и $S_t \notin K$ выполняется $F(U(S_i)) \neq F(U(S_t))$, называется {\it корректирующей} для $U$.

\section{Логические корректоры}
В основе идеи логического корректора лежит алгебро-логический подход, сочетающий в себе идеи алгебраического(см. \cite{alg_meth}) и логического подходов. Алгебраический подход позволяет скорректировать несколько базовых алгоритмов, каждый из которых безошибочно распознаёт лишь часть объектов. Это и используется в алгебро-логическом подходе, когда строится корректная процедура распознавания на базе эл.кл., которые не обязательно являются корректными. 


Простейший логический корректор был впервые построен в \cite{sotnezov11} 

\subsection{Принцип работы логического корректора}

Общую схему работы логического корректора можно разбить на два этапа: этап обучения и этап распознавания.

{\bf Этап обучения:}

Для каждого объекта $K$, $K \in \lbrace K_1, ..., K_l \rbrace$, распознающий алгоритм $A$ строит семейство корректирующих наборов эл.кл. $W_A(K)$, после чего каждому корректирующему набору эл.кл. $U$, $U \in W_A(K)$ приписывается некоторый вес $\alpha_U$.

{\bf Этап распознавания:}

Для каждого распознаваемого объекта $S$ вычисляется оценка принадлежности этого объекта к каждому из классов $K$ по формуле:
\[
\Gamma_K^{LC} (S) = \sum_{U \in W_A(K)} \alpha_U F_{U, K}(U(S)),
\]
где $F_{U, K}$ - корректирующая булева функция для набора $U$.

В простейшем случае все наборы имеют одинаковые веса, тогда формула выше сведётся к следующему виду:
\[
\Gamma_K^{LC} (S) = \frac{1}{|W_A(K)|} \sum_{U \in W_A(K)} F_{U, K}(U(S)).
\]
В итоге, алгоритм относит неизвестный объект S к тому классу, для которого оценка принадлежности будет максимальной.

\subsection{Алгоритм MONS}
Одним из первых логических корректоров, показывающих хорошие результаты, являлся алгоритм MON, который использовал монотонные корректные наборы из эл.кл. ранга 1. Его функция голосования использует свойство монотонности наборов и сама является монотонной:
\[
F_{U, K}^{MON} (S) = \frac{1}{T \cap K} \sum_{S' \in T \cap K} \delta_U(S, S'),
\]
где $\delta_U(S', S'')$ является функцией голосования и равна
\[
\delta_U(S', S'') = \begin{cases}
1, & \text{если $\omega_U(S') \succeq \omega_U(S'')$;} \\
0, & \text{иначе.}
\end{cases}
\]

Алгоритм MONS является улучшенной моделью алгоритма MON, который использует т.н. {\it локальный базис} --- совокупность эл.кл., на основе которых базовые алгоритмы строят наборы эл.кл. Использование локального базиса является компромиссом между желанием использовать в логическом корректоре эл.кл. ранга больше 1 и трудоёмкостью вычислений при поиске корректных монотонных наборов эл.кл. Однако даже формирование самого локального базиса само по себе является непростой задачей, и каждый алгоритм решает её по-разному. В нашем случае MONS использует стохастические методы, каждый раз формируя случайно небольшой локальный базис и на его основании составляя новые наборы эл.кл., которые добавляются к растущим семействам наборов.

Для поиска наборов эл.кл. используется генетический алгоритм, который подробно описан в \cite{sotnezov11}. Если описать его вкратце, то мы кодируем значения эл.кл. на парах разных объектов из обучающей выборки в виде матрицы, сводя нашу задачу к задаче дуализации --- задаче поиска неприводимого покрытия в булевой матрице(более подробно см. \cite{djukova_stuff})

\begin{algorithm}
\caption{Процедура обучения алгоритма MONS}
\begin{algorithmic}[1]
\REQUIRE $T$ --- обучающая выборка, $max\_i$ --- максимальное число итераций, $\varepsilon$ --- порог;
\ENSURE $W_K, K \in \lbrace K_1, \dots, K_l \rbrace$ --- семейства мон. классифицирующих наборов;
\STATE $V := V(T)$; \COMMENT{случайное выделение выборки $V \subset T$ в качестве валидационной}
\STATE $T := T \backslash V$; \COMMENT{усечение обучающей выборки}
\FOR{$i=1,\dots, max\_i$}
	\FORALL{$K \in \lbrace K_1, \dots, K_l \rbrace $}
		\STATE $LB := LB()$; \COMMENT{случайным образом сформировать локальный базис}
		\STATE $W_K^i := GA(T, LB, K)$; \COMMENT{ГА возвращает семейство мон. наборов из LB}
		\STATE $W_K := W_K \cap W_K^i$; \COMMENT{добавляем полученные наборы в семейство $W_K$}
	\ENDFOR
	\FORALL {$S \in V$}
		\STATE $M_i(S) := \Gamma^{MON}_{y(S)}(W^i_{y(S)}, S) - \max\limits_{K \in \lbrace K_1, \dots, K_l \rbrace \backslash y(S)}\Gamma^{MON}_K(W^i_K, S)  $ \COMMENT{отступ}
		\STATE $M^{avg}_i(S) := \frac{1}{i} \sum_{j = i}^i M_j(S)$	; \COMMENT{средний отступ по предыдущим итерациям}
	\ENDFOR
	\IF {$\forall S \in V M_i^{avg}(S) - M_{i-1}^{avg}(S) < \varepsilon$}
		\STATE {\bf выход}
	\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}


\newpage
\subsection{Сведение к задаче восстановления регрессии}
Для применения алгоритма MONS к задаче восстановления регрессии предлагается адаптировать нашу задачу к задаче классификации. Мы можем либо исходить из предположения, что у похожих объектов близкие значения целевой переменной, либо не допускать подобного. В первом случае логично разбить объекты по их признаковым описаниям на несколько кластеров, каждому из которых будет присвоена своя метка класса, и сведём задачу к предыдущей. Во втором случае мы можем разбить на классы исходя только из значения самой целевой переменной. 

Для разбиения данных на кластеры можно использовать разные алгоритмы, но важно, чтобы алгоритм умел самостоятельно определять количество кластеров, присутствующих в выборке, отфильтровывал выбросы, умел работать с нелинейной геометрией данных а также был быстрым и простым в реализации. Таким параметрам удовлетворяет алгоритм DM-DBSCAN\cite{dmdbscan} --- динамический плотностной алгоритм кластеризации пространственных данных с присутствием шума. Он, как и его предшественник DBSCAN, основывается на предположении, что внутри каждого кластера наблюдается типичная плотность объектов, которая заметно выше, чем плотность снаружи кластера, а плотность в областях с шумом ниже плотности любого из кластеров. Однако DBSCAN требует в качестве параметра значение плотности кластера, которое меняется в зависимости от исходных данных, и не умеет обрабатывать кластеры с различной плотностью. DM-DBSCAN лишён этих недостатков, т.к. оценивает по графику кривой расстояний до k-го ближайшего соседа --- графику k-расстояний --- уровни плотности каждого из кластеров.   

После разбиения на кластеры, мы используем алгоритм MONS для решения уже знакомой задачи классификации, настраивая модель. При распознавании нового объекта алгоритм MONS в качестве ответа выдаст один из кластеров, к которому объект, скорее всего, принадлежит. Найдём значение целевой функции объекта, основываясь на значениях объектов из данного кластера. 
\newpage
Это можно сделать несколькими способами:
\begin{itemize}
	\item среднее значение/медиана всех значений целевой переменной в классе
	\item среднее значение/медиана значений целевой переменной у некоторых $d$ объектов в нашемм классе, которые наиболее похожи на тестируемый --- можно использовать разные метрики, в зависимости от типа данных
\end{itemize}

\section{Вычислительные эксперименты}

%Цель данного раздела:
%продемонстрировать, что предложенная теория работает на практике;
%показать границы её применимости;
%рассказать о~новых экспериментальных фактах.

%Чисто теоретические работы могут вообще не~содержать раздела экспериментов
%(не~работает, ну и~не~надо~--- зато теория красивая).
%Кстати, теоретики имеют право не~догадываться, где, кому и~когда их теории пригодятся.

\subsection{Исходные данные и~условия эксперимента}
%Описывается прикладная задача, параметры анализируемых данных 
%(например, сколько объектов, сколько признаков, каких они типов), 
%параметры эксперимента 
%(например, как производился скользящий контроль). 

\subsection{Результаты эксперимента}
%Результаты экспериментов представляются в~виде таблиц и~графиков. 
%Объясняется точный смысл всех обозначений на графиках, строк и~столбцов в~таблицах. 

\subsection{Выводы}
%Приводятся выводы: 
%в~какой степени результаты экспериментов согласуются с~теорией? 
%Достигнут ли желаемый результат? 
%Обнаружены ли какие-либо факты, не~нашедшие объяснения, и~которые нельзя списать на «грязный» эксперимент?

%Обсуждаются основные отличия предложенных методов от известных ранее. 
%В~чем их преимущества? 
%Каковы границы их применимости? 
%Какие проблемы удалось решить, а~какие остались открытыми? 
%Какие возникли новые постановки задач?

\section{Заключение}

%В~квалификационных работах последний раздел нужен для того, чтобы 
%конспективно перечислить основные результаты, полученные лично автором. 

\renewcommand{\bibname}{Список литературы}
\addcontentsline{toc}{section}{\bibname}

\nocite{hastie09elements,bishop06pattern,zhuravlev06recognition,zhuravlev78prob33,shlezinger04ten,boucheron05theory}

\def\BibUrl#1.{}\def\BibAnnote#1.{}
%\def\BibUrl#1{\\{\footnotesize\tt\def~{\char126} http://#1}}
\bibliographystyle{gost71s}
\bibliography{MachLearn}
\begin{thebibliography}{0}
\bibitem{main_source}{\it Е. В. Дюкова, Ю. И. Журавлёв, К. В. Рудаков} Об алгебро-логическом синтезе корректных процедур распознавания на базе элементарных алгоритмов // Журнал вычислительной математики и математической физики, 1996, 36:8   215–223

\bibitem{alg_meth} {\it Ю. И. Журавлёв} Об алгебраическом подходе к решению задач распознавания или классификации // Проблемы кибернетики, 1978, вып. 33, 5–68

\bibitem{lyubimtseva14} {\it Любимцева М.М., Дюкова Е.В.} Логические корректоры в задачах распознавания // 2014

\bibitem{sotnezov11} {\it E.V. Djukova, Yu.I. Zhuravlev, R.M. Sotnezov.} Construction of an Ensemble of Logical Correctors on the Basis of Elementary Classifiers // Pattern Recognition and Image Analysis, 2011, Vol. 21, No4, pp. 599—605.

\bibitem{sotnezov09} {\it R. M. Sotnezov.} Genetic Algorithms for Problems of Logical Data Analysis in Discrete Optimization and Image Recognition // Pattern Recognition and Image Analysis, 2009, Vol. 19, No. 3, 469–477

\bibitem{trees} {\it Wei-Yin Loh} Classification and regression trees // Wiley Interdisciplinary Reviews:
Data Mining and Knowledge Discovery Vol.1, 2011, DOI: 10.1002/widm.8

\bibitem{dmdbscan} {\it M.T.H. Elbatta, W.M. Ashour} A Dynamic Method for Discovering Density Varied Clusters // International Journal of Signal Processing, Image Processing and Pattern Recognition Vol.6, No. 1, 2013 

\end{thebibliography}

\end{document}
